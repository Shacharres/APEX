{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440e0d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41475324",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"To check our implementation, we will try to load the GPT-2 weights from huggingface, alongside the official implementation \n",
    "from the `transformers` library, and see if we get the same token probabilities for an example prompt.\n",
    "\"\"\"\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "official_gpt_model = GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\")\n",
    "\n",
    "our_gpt_model = GPT.from_pretrained(\"openai-community/gpt2\")\n",
    "\n",
    "prompt = \"Have you ever seen the rain?\"\n",
    "token_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "our_logits = our_gpt_model(token_ids)\n",
    "official_logits = official_gpt_model(token_ids).logits\n",
    "\n",
    "print(\"Our logits:\")\n",
    "print(our_logits[0, -1, :10])\n",
    "print(\"Official logits:\")\n",
    "print(official_logits[0, -1, :10])\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Mean difference (this should be close to 0):\")\n",
    "print((our_logits - official_logits).abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775e1267",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generation\n",
    "Implement top-k sampling: from the token probabilities, keep only the top k, and sample one of them according to the probabilities\n",
    "\n",
    "compare generated samples from the gpt model you implemented to the official gpt implementation. Are they both intelligible?\n",
    "\"\"\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # You can configure colab to run on a T4 GPU for faster generation\n",
    "tokens = [15496, 11, 314, 1101, 257, 3303, 2746, 11] # \"Hello, I'm a language model,\"\n",
    "tokens = torch.tensor(tokens, dtype=torch.long) # (8,)\n",
    "tokens = tokens.unsqueeze(0).repeat(5, 1) # Generate in a batch of 5. Shape is (5, 8)\n",
    "x = tokens.to(device)\n",
    "\n",
    "# Move the model to the correct device\n",
    "our_gpt_model.to(device)\n",
    "\n",
    "k = 20\n",
    "\n",
    "print(\"Generating on device:\", device)\n",
    "# generate!\n",
    "our_gpt_model.eval()\n",
    "while x.size(1) < 30: # max_length=30\n",
    "    # forward the model to get the logits\n",
    "    with torch.no_grad():\n",
    "        logits = our_gpt_model(x)\n",
    "        # only care about the last token\n",
    "        logits = logits[:, -1, :] # Shape (batch, vocab_size)\n",
    "\n",
    "        # Implement top-k masking: set non-top-k logits to -inf\n",
    "        topk_values, _ = torch.topk(logits, k, dim=-1)\n",
    "        kth_largest_value = topk_values[:, -1].unsqueeze(-1) # shape (batch, 1)\n",
    "        logits_masked = torch.where(logits < kth_largest_value, torch.full_like(logits, float('-inf')), logits)\n",
    "\n",
    "        # now do softmax\n",
    "        probs = F.softmax(logits_masked, dim=-1) # Softmax on (batch, vocab_size)\n",
    "\n",
    "        # sample according to prob\n",
    "        next = torch.multinomial(probs, num_samples=1)\n",
    "        x = torch.cat((x, next), dim=1)\n",
    "\n",
    "print(tokenizer.batch_decode(x))\n",
    "[print(s) for s in tokenizer.batch_decode(x)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
